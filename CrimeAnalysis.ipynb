{"cells":[{"cell_type":"markdown","source":["# Import some useful computing libraries"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function\n\nfrom datetime import datetime, date, timedelta\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import udf"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# Useful commands"],"metadata":{}},{"cell_type":"code","source":["# Close all figures to release memory\nplt.close(\"all\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Define some constants here"],"metadata":{}},{"cell_type":"code","source":["# Interval we are interested\nSTARTTIME = datetime(2014, 1, 1, 0, 0, 0)\nENDTIME = datetime(2017, 5, 1, 0, 0, 0)\n\n# The following constants are derviated from the above constants\nSTARTDATE = STARTTIME.date()\nENDDATE = ENDTIME.date()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# Get Data that stored in S3"],"metadata":{}},{"cell_type":"code","source":["ACCESSY_KEY_ID = u\"AKIAI7XH3QZ3MFK54IKQ\"\nSECERET_ACCESS_KEY = u\"Vte/7rTdng469SgbsKkkSTNFH1Tij3CGonVU4hhI\"\n\nmounts_list = [\n{'bucket':'comp4651aaron-us/', 'mount_folder':'/mnt/comp4651'}\n]\n\nprint(\"This cell contains a secret\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["for mount_point in mounts_list:\n  bucket = mount_point['bucket']\n  mount_folder = mount_point['mount_folder']\n  try:\n    dbutils.fs.unmount(mount_folder)\n  except:\n    pass\n  finally: #If MOUNT_FOLDER does not exist\n    import urllib\n    dbutils.fs.mount(\"s3a://\"+ urllib.quote_plus(ACCESSY_KEY_ID) + \":\" + urllib.quote_plus(SECERET_ACCESS_KEY) + \"@\" + bucket,mount_folder)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%fs ls /mnt/comp4651"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Import csv file into raw dataframes"],"metadata":{}},{"cell_type":"code","source":["datafileMC = '/mnt/comp4651/Crime.fixed.csv'\ndatafileSF = '/mnt/comp4651/Police_Department_Incidents.csv'\n\ndfMCraw = spark.read.csv(datafileMC, header=True, inferSchema=True)\ndfSFraw = spark.read.csv(datafileSF, header=True, inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Let's take a look of the original columns and a few rows in the raw dataframes"],"metadata":{}},{"cell_type":"code","source":["print(\"MC\")\ndfMCraw.printSchema()\nprint(dfMCraw.take(2))\n\nprint(\"SF\")\ndfSFraw.printSchema()\nprint(dfSFraw.take(2))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["catemapFileMC = 'dbfs:/mnt/comp4651/mtCrime.csv'\ncatemapFileSF = 'dbfs:/mnt/comp4651/sfCrime.csv'\n\ndef loadCatemapMC(csvfile):\n  from HTMLParser import HTMLParser\n  htmlparser = HTMLParser()\n  \n  schema = StructType([\n    StructField(\"Class_Description\", StringType(), False),\n    StructField(\"count\", IntegerType(), False),\n    StructField(\"New_Class\", StringType(), False),\n  ])\n  \n  df = spark.read.csv(csvfile, header=True, schema=schema)\n  pairs = df.select(\"Class_Description\", \"New_Class\").collect()\n  \n  catemap = {htmlparser.unescape(s[\"Class_Description\"]): htmlparser.unescape(s[\"New_Class\"]) for s in pairs}\n  return catemap\n\ndef loadCatemapSF(csvfile):\n  from HTMLParser import HTMLParser\n  htmlparser = HTMLParser()\n  \n  schema = StructType([\n      StructField(\"Category\", StringType(), False),\n      StructField(\"Descript\", StringType(), False),\n      StructField(\"count\", IntegerType(), False),\n      StructField(\"New_Class\", StringType(), False),\n    ])\n  \n  df = spark.read.csv(csvfile, header=True, schema=schema)\n  pairs = df.select(\"Category\", \"Descript\", \"New_Class\").collect()\n  catemap = {(htmlparser.unescape(s[\"Category\"]), htmlparser.unescape(s[\"Descript\"])): htmlparser.unescape(s[\"New_Class\"]) for s in pairs}\n  return catemap\n\ncatemapMC = loadCatemapMC(catemapFileMC)\ncatemapSF = loadCatemapSF(catemapFileSF)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Find un-mapped category\n# MC\nsetA_MC = set(dfMCraw.rdd.map(lambda s: s[\"Class Description\"]).distinct().collect())\nsetB_MC = set(catemapMC.keys())\nprint(setA_MC.difference(setB_MC), sep='\\n')\nprint(setB_MC.difference(setA_MC), sep='\\n')"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Find un-mapped category\n# SF\nsetA_SF = set(dfSFraw.rdd.map(lambda s: (s[\"Category\"], s[\"Descript\"])).distinct().collect())\nsetB_SF = set(catemapSF.keys())\nprint(setA_SF.difference(setB_SF), sep='\\n')\nprint(setB_SF.difference(setA_SF), sep='\\n')"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["Selecting the interested columns and transfer the raw dataframes to the normalized dataframes"],"metadata":{}},{"cell_type":"code","source":["schema = StructType([\n    StructField(\"Id\", IntegerType(), False),\n    StructField(\"DataSource\", StringType(), False),\n    StructField(\"Category\", StringType(), True),\n    StructField(\"Datetime\", TimestampType(), False),\n    StructField(\"Hour\", IntegerType(), False),\n    StructField(\"Month\", IntegerType(), False),\n    StructField(\"Year\", IntegerType(), False),\n    StructField(\"DayOfWeek\", IntegerType(), False),\n    StructField(\"MonthDelta\", IntegerType(), False),\n    StructField(\"TimeCategory\", StringType(), False),\n  ])\n\nCrimeRow = Row(\n  \"Id\",\n  \"DataSource\",\n  \"Category\",\n  \"Datetime\",\n  \"Hour\",\n  \"Month\",\n  \"Year\",\n  \"DayOfWeek\",\n  \"MonthDelta\",\n  \"TimeCategory\",\n)\n\ndef toMonthDelta(dt):\n  return (dt.year - STARTTIME.year) * 12 + (dt.month - STARTTIME.month)\n\ndef toTimeCategory(dt):\n  hour = dt.hour\n  if 6 <= hour <= 11:\n    tc = \"Morning\"\n  elif 12 <= hour <= 17:\n    tc = \"Afternoon\"\n  elif 18 <= hour <= 23:\n    tc = \"Night\"\n  else:\n    tc = \"Evening\"\n  return tc"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["def schemaNormalizerMC(s):\n  rowId = s[\"Incident ID\"]\n  datasource = \"MC\"\n  old_category = s[\"Class Description\"]\n  category = catemapMC.get(old_category, None)\n  dt = datetime.strptime(s[\"Dispatch Date / Time\"], '%m/%d/%Y %I:%M:%S %p')\n  (hour, month, year) = (dt.hour, dt.month, dt.year)\n  dayofweek = dt.isoweekday()\n  monthDelta = toMonthDelta(dt)\n  tc = toTimeCategory(dt)\n  return CrimeRow(rowId, datasource, category, dt, hour, month, year, dayofweek, monthDelta, tc)\n\ndfMC = spark.createDataFrame(dfMCraw.rdd.map(schemaNormalizerMC), schema)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["def schemaNormalizerSF(s):\n  rowId = s[\"IncidntNum\"]\n  datasource = \"SF\"\n  old_category = s[\"Category\"]\n  old_descript = s[\"Descript\"]\n  category = catemapSF.get((old_category, old_descript), None)\n  dt = datetime.strptime(\"{} {}\".format(s[\"Date\"], s[\"Time\"]) , '%m/%d/%Y %H:%M')\n  (hour, month, year) = (dt.hour, dt.month, dt.year)\n  dayofweek = dt.isoweekday()\n  monthDelta = toMonthDelta(dt)\n  tc = toTimeCategory(dt)\n  return CrimeRow(rowId, datasource, category, dt, hour, month, year, dayofweek, monthDelta, tc)\n\ndfSF = spark.createDataFrame(dfSFraw.rdd.map(schemaNormalizerSF), schema)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["Filter the normalized dataframes between 1st, Jan 2014 and 1st, May 2017"],"metadata":{}},{"cell_type":"code","source":["# Filter dataframe and cache the resultant dataframe\ndef filterDataFrame(df):\n  df = df.filter((df[\"Datetime\"] >= STARTTIME) & (df[\"Datetime\"] < ENDTIME) \n                 & (df[\"Category\"] != \"NON-CRIMINAL\")\n                 & (df[\"Category\"] != \"AIDED CASE\")\n                 & (df[\"Category\"] != \"SUSPICIOUS OCC\")\n                 & (df[\"Category\"] != \"TRAFFIC VIOLATION\")\n                 & (df[\"Category\"] != \"POL INFORMATION\")\n                 & (df[\"Category\"] != \"OTHER OFFENSES\")\n                 & (df[\"Category\"] != \"MISSING PERSON\")\n                )\n  return df\n\ndfMC = filterDataFrame(dfMC).cache()\ndfSF = filterDataFrame(dfSF).cache()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Actually this cell is used to test the above code is 100% working\nprint(\"After filtering, there are {} rows in MC and {} rows in SF\".format(dfMC.count(), dfSF.count()))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["\nprint(\"Categories are:\")\nprint(\"\\n\".join([s[\"Category\"] for s in dfMC.union(dfSF).select(\"Category\").distinct().collect()]))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Take a look of the first and the last row of each dataframe in a table"],"metadata":{}},{"cell_type":"code","source":["def firstAndLastRows(df):\n  first = df.orderBy(\"Datetime\", ascending=True).limit(1)\n  last = df.orderBy(\"Datetime\", ascending=False).limit(1)\n  return first.union(last)\n\ndisplay(firstAndLastRows(dfMC).union(firstAndLastRows(dfSF)).collect())"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import *\n\ndef calYearDelta(year):\n  return year - 2014\n\nudfYearDelta = udf(calYearDelta, IntegerType())\n\ncatIndexer = StringIndexer(inputCol=\"Category\", outputCol=\"categoryIndex\")\ntrainedCatIndexer = catIndexer.fit(dfSF.union(dfMC))\nindexedSF = trainedCatIndexer.transform(dfSF)\nindexedMC = trainedCatIndexer.transform(dfMC)\nindexedMC = indexedMC.withColumn(\"YearDelta\", udfYearDelta(\"Year\"))\nindexedSF = indexedSF.withColumn(\"YearDelta\", udfYearDelta(\"Year\"))\ndisplay(indexedMC.limit(3).union(indexedSF.limit(3)))\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["indexedMC.printSchema()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["from pyspark.sql.functions import *\ndef monthCountMap(cat, count):\n  i = 0\n  while 100*i<=count:\n    i += 1\n  return cat+\" - \"+ str(i)\n\nudfMonthCountMap = udf(monthCountMap, StringType())\nfpMonthMC = indexedMC.groupBy(\"MonthDelta\",\"Category\").count().withColumn(\"countMonthMap\", udfMonthCountMap(\"Category\", \"count\")).select(\"MonthDelta\", \"countMonthMap\").groupBy(\"MonthDelta\").agg(collect_list(\"countMonthMap\").alias(\"catCount\"))\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["fpMonthSF = indexedSF.groupBy(\"MonthDelta\",\"Category\").count().withColumn(\"countMonthMap\", udfMonthCountMap(\"Category\", \"count\")).select(\"MonthDelta\", \"countMonthMap\").groupBy(\"MonthDelta\").agg(collect_list(\"countMonthMap\").alias(\"catCount\"))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(fpMonthMC)\ndisplay(fpMonthSF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["#for x in fpMonthMC:\n  #print(x.catCount)\nfpMonthMC2 = fpMonthMC.rdd.map(lambda row: row.catCount)#[x.catCount for x in fpMonthMC]\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["fpMonthSF2 = fpMonthSF.rdd.map(lambda row: row.catCount)#[x.catCount for x in fpMonthMC]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.mllib.fpm import FPGrowth\n\nmodelMonthMC = FPGrowth.train(fpMonthMC2, minSupport=0.7, numPartitions=5)\nresultMonthMC = modelMonthMC.freqItemsets()\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["\nmodelMonthSF = FPGrowth.train(fpMonthMC2, minSupport=0.7, numPartitions=5)\nresultMonthSF = modelMonthSF.freqItemsets()\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["monthlyFreqItemsMC = resultMonthMC.collect()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["monthlyFreqItemsSF = resultMonthSF.collect()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["print(monthlyFreqItemsSF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["monthlyFreqItemsSortMC1 = sorted(monthlyFreqItemsMC, key=lambda x: len(x.items), reverse=True)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["monthlyFreqItemsSortSF1 = sorted(monthlyFreqItemsSF, key=lambda x: len(x.items), reverse=True)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["resultMonthFreqMC1 = []\nfor i in monthlyFreqItemsSortMC1:\n  included = False\n  for j in resultMonthFreqMC1:\n    if set(i.items).issubset(set(j.items)):\n      included = True\n  if not included: \n    resultMonthFreqMC1.append(i)\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["resultMonthFreqSF1 = []\nfor i in monthlyFreqItemsSortSF1:\n  included = False\n  for j in resultMonthFreqSF1:\n    if set(i.items).issubset(set(j.items)):\n      included = True\n  if not included: \n    resultMonthFreqSF1.append(i)\n"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["print(resultMonthFreqSF1)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols=[\"Month\", \"DayOfWeek\", \"Hour\"],#, \"YearDelta\"],\n    outputCol=\"features\")\n\ntransformedMC = assembler.transform(indexedMC)\ntransformedSF = assembler.transform(indexedSF)\ndisplay(transformedMC.limit(3))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n\nprepMC = transformedMC.rdd.map(lambda row: LabeledPoint(row.categoryIndex, Vectors.dense(row.features)))\nnaiveBayesMC = NaiveBayes.train(prepMC)\nprepSF = transformedSF.rdd.map(lambda row: LabeledPoint(row.categoryIndex, Vectors.dense(row.features)))\nnaiveBayesSF = NaiveBayes.train(prepSF)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["for i in xrange(0,24):\n  print(naiveBayesMC.predict(Vectors.dense([2,8,i])))"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["transformedSF.filter(transformedSF.categoryIndex == 6).select(\"Category\", \"categoryIndex\").take(5)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["import pyspark.sql\n\nprint(\"transformedMC.corr\", transformedMC.corr(\"categoryIndex\",\"timeCategoryIndex\"))\nprint(\"transformedMC.corr\", transformedMC.corr(\"categoryIndex\",\"Month\"))\nprint(\"transformedMC.corr\", transformedMC.corr(\"categoryIndex\",\"Year\"))\nprint(\"transformedMC.corr\", transformedMC.corr(\"categoryIndex\",\"DayOfWeek\"))\nprint(\"transformedMC.cov\", transformedMC.cov(\"categoryIndex\",\"timeCategoryIndex\"))\nprint(\"transformedMC.cov\", transformedMC.cov(\"categoryIndex\",\"Month\"))\nprint(\"transformedMC.cov\", transformedMC.cov(\"categoryIndex\",\"Year\"))\nprint(\"transformedMC.cov\", transformedMC.cov(\"categoryIndex\",\"DayOfWeek\"))\nprint(\"transformedSF.corr\", transformedSF.corr(\"categoryIndex\",\"timeCategoryIndex\"))\nprint(\"transformedSF.corr\", transformedSF.corr(\"categoryIndex\",\"Month\"))\nprint(\"transformedSF.corr\", transformedSF.corr(\"categoryIndex\",\"Year\"))\nprint(\"transformedSF.corr\", transformedSF.corr(\"categoryIndex\",\"DayOfWeek\"))\nprint(\"transformedSF.cov\", transformedSF.cov(\"categoryIndex\",\"timeCategoryIndex\"))\nprint(\"transformedSF.cov\", transformedSF.cov(\"categoryIndex\",\"Month\"))\nprint(\"transformedSF.cov\", transformedSF.cov(\"categoryIndex\",\"Year\"))\nprint(\"transformedSF.cov\", transformedSF.cov(\"categoryIndex\",\"DayOfWeek\"))\n#display(transformedMC.crosstab(\"categoryIndex\",\"timeCategoryIndex\"))\n#display(transformedMC.crosstab(\"categoryIndex\",\"Month\"))\n#display(transformedMC.crosstab(\"categoryIndex\",\"Year\"))\n#display(transformedMC.crosstab(\"categoryIndex\",\"DayOfWeek\"))\n#freqItems\n#display(transformedMC.freqItems([\"categoryIndex\",\"timeCategoryIndex\", \"Month\", \"Year\", \"DayOfWeek\"]))\n#print(transformedMC.freqItems(\"categoryIndex\",\"Month\"))\n#print(transformedMC.freqItems(\"categoryIndex\",\"Year\"))\n#print(transformedMC.freqItems(\"categoryIndex\",\"DayOfWeek\"))"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["Table of top 20 category for the MC."],"metadata":{}},{"cell_type":"code","source":["display(transformedMC.groupBy(\"Category\").count().orderBy(\"count\", ascending=False).limit(20).collect())"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["display(transformedSF.groupBy(\"Category\").count().orderBy(\"count\", ascending=False).limit(20).collect())"],"metadata":{},"outputs":[],"execution_count":52}],"metadata":{"name":"AaronAnalysis","notebookId":1871842621754004},"nbformat":4,"nbformat_minor":0}
